# %%capture
# ================= 0) Install =================
!pip -q install "torch>=2.2" --index-url https://download.pytorch.org/whl/cu121
!pip -q install transformers accelerate datasets peft bitsandbytes gradio

# ================= 1) Train (regex pseudo-labels) =================
import os, math, re, torch, random
from dataclasses import dataclass
from typing import Dict, List
from datasets import load_dataset, DatasetDict
from accelerate import Accelerator, DistributedDataParallelKwargs
from transformers import (AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig,
                          get_scheduler, set_seed)
from torch.utils.data import DataLoader
from tqdm import tqdm
import peft as p

# ---- Config ----
MODEL_NAME   = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
DATASET_NAME = "ai4privacy/pii-masking-200k"
OUTPUT_DIR   = "./pie_chat_model_regex"
LIMIT_TRAIN  = 1000   # keep small for smoke; raise later if you like
LIMIT_EVAL   = 200
MAX_LENGTH   = 1024
EPOCHS       = 10     # <â€” as you asked
BATCH_SIZE   = 4
LR           = 2e-5
WARMUP_RATIO = 0.06
PRECISION    = "bf16"    # swap to "fp16" if needed
QUANT        = "4bit"    # QLoRA
FINETUNE     = "lora"
LANGUAGE     = "en"      # keep English if a lang key exists

INSTR = "Instruction: Mask all PII (names, emails, phones, addresses, postal codes, SSNs, IDs, etc.)."
PROMPT_FMT = INSTR + "\nText: {orig}\nAnswer: "

set_seed(42)

# ---- Regex-based pseudo-labeler (no spans required) ----
# NOTE: These are conservative and meant for training a demo redactor.
# You can tighten/expand as needed.
EMAIL_RE   = re.compile(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}\b')
PHONE_RE   = re.compile(r'(?:(?:\+?\d{1,3}[\s.-]?)?(?:\(?\d{2,4}\)?[\s.-]?)?\d{3}[\s.-]?\d{4,5})')
SSN_RE     = re.compile(r'\b\d{3}-\d{2}-\d{4}\b')
ZIP_RE     = re.compile(r'\b\d{5}(?:-\d{4})?\b|\b[A-Z]\d[A-Z]\s?\d[A-Z]\d\b', re.IGNORECASE)  # US + loose CA
DATE_RE    = re.compile(r'\b(?:\d{1,2}[/.-]\d{1,2}[/.-]\d{2,4}|\d{4}-\d{2}-\d{2}|(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\s+\d{1,2},?\s+\d{2,4})\b', re.IGNORECASE)
ID_RE      = re.compile(r'\b(?:MRN|ID|DL|License|Passport)\s*[:#]?\s*[A-Za-z0-9\-]{4,}\b', re.IGNORECASE)
ADDR_KWS   = r'(?:street|st\.?|road|rd\.?|avenue|ave\.?|boulevard|blvd\.?|lane|ln\.?|drive|dr\.?|terrace|way|court|ct\.?)'
ADDRESS_RE = re.compile(r'\b\d{1,5}\s+[A-Za-z0-9 .\'-]+?\s+' + ADDR_KWS + r'\b(?:[, ]+[A-Za-z .\'-]+){0,3}', re.IGNORECASE)

# simple name heuristic: two capitalized words in a row (avoid months/days/common stopwords)
STOP = set("Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec Mon Tue Wed Thu Fri Sat Sun The A An And Or Of For With".split())
NAME_RE = re.compile(r'\b([A-Z][a-z]+)\s+([A-Z][a-z]+)\b')

def mask_with_regex(text: str) -> str:
    if not isinstance(text, str) or not text.strip():
        return text

    out = text
    out = EMAIL_RE.sub("<EMAIL>", out)
    out = SSN_RE.sub("<SSN>", out)
    out = ID_RE.sub("<ID>", out)
    out = DATE_RE.sub("<DATE>", out)
    out = ZIP_RE.sub("<ZIP>", out)
    out = ADDRESS_RE.sub("<ADDRESS>", out)

    # Phones last to avoid clobbering IDs etc.
    out = PHONE_RE.sub("<PHONE>", out)

    # Naive NAME masking: only if still looks like two proper nouns and not in STOP
    def _name_repl(m):
        w1, w2 = m.group(1), m.group(2)
        if w1 in STOP or w2 in STOP:
            return m.group(0)
        return "<NAME>"

    out = NAME_RE.sub(_name_repl, out)
    return out

# ---- Data helpers ----
def detect_lang_key(ex):
    for k in ("language","lang","locale"):
        if k in ex: return k
    return None

def choose_text_key(ex):
    for k in ("text","original","input","source","raw"):
        if k in ex and isinstance(ex[k], str) and ex[k].strip():
            return k
    # fallback: first string field
    for k,v in ex.items():
        if isinstance(v, str) and v.strip():
            return k
    return None

@dataclass
class RedactionItem:
    input_ids: torch.Tensor
    attention_mask: torch.Tensor
    labels: torch.Tensor

class PiiRegexDataset(torch.utils.data.Dataset):
    """Build targets from regex redaction (no 'template', no 'spans')."""
    def __init__(self, hf_split, tokenizer, max_length):
        self.hf_split, self.tok, self.max_length = hf_split, tokenizer, max_length
        self.text_key = choose_text_key(hf_split[0])

    def __len__(self): return len(self.hf_split)

    def __getitem__(self, i):
        ex   = self.hf_split[i]
        text = ex.get(self.text_key, "")
        target = mask_with_regex(text)

        prompt = PROMPT_FMT.format(orig=text)
        full   = prompt + target

        t = self.tok(full, max_length=self.max_length, truncation=True,
                     padding="max_length", return_tensors="pt")
        input_ids = t["input_ids"].squeeze(0)
        attn_mask = t["attention_mask"].squeeze(0)

        prompt_ids = self.tok(prompt, max_length=self.max_length, truncation=True,
                              padding=False, return_tensors="pt")["input_ids"].squeeze(0)

        labels = input_ids.clone()
        # ignore prompt
        labels[:min(prompt_ids.shape[0], self.max_length)] = -100
        # ignore padding
        labels[attn_mask == 0] = -100
        return {"input_ids": input_ids, "attention_mask": attn_mask, "labels": labels}

# ---- Accelerator / dtype / quant ----
ddp = DistributedDataParallelKwargs(find_unused_parameters=True, static_graph=True)
accelerator = Accelerator(mixed_precision=(PRECISION if PRECISION!="none" else "no"),
                          kwargs_handlers=[ddp])

dtype = torch.bfloat16 if PRECISION=="bf16" else (torch.float16 if PRECISION=="fp16" else torch.float32)
bnb = None
if QUANT == "4bit":
    bnb = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True,
                             bnb_4bit_quant_type="nf4", bnb_4bit_compute_dtype=dtype)
elif QUANT == "8bit":
    bnb = BitsAndBytesConfig(load_in_8bit=True, bnb_8bit_compute_dtype=dtype)

# ---- Load base model ----
tok = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True, use_fast=True)
if tok.pad_token is None: tok.pad_token = tok.eos_token
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, dtype=dtype, quantization_config=bnb, trust_remote_code=True)

# LoRA
if FINETUNE == "lora":
    if QUANT != "none":
        model = p.prepare_model_for_kbit_training(model)
    targets = ["q_proj","k_proj","v_proj","o_proj","gate_proj","up_proj","down_proj"]
    lcfg = p.LoraConfig(r=8, lora_alpha=16, lora_dropout=0.05, bias="none",
                        task_type="CAUSAL_LM", target_modules=targets)
    model = p.get_peft_model(model, lcfg)

# ---- Load dataset; keep English if available; build regex targets ----
ds: DatasetDict = load_dataset(DATASET_NAME)

# choose splits
if "train" in ds and "validation" in ds:
    train_raw, eval_raw = ds["train"], ds["validation"]
elif "train" in ds:
    s = ds["train"].train_test_split(test_size=0.1, seed=42)
    train_raw, eval_raw = s["train"], s["test"]
else:
    k0 = list(ds.keys())[0]
    s = ds[k0].train_test_split(test_size=0.1, seed=42)
    train_raw, eval_raw = s["train"], s["test"]

# Filter to English (if such a column exists)
lk = detect_lang_key(train_raw[0]) or detect_lang_key(eval_raw[0])
if lk and LANGUAGE:
    train_raw = train_raw.filter(lambda ex: str(ex.get(lk, "")).lower().startswith(LANGUAGE))
    eval_raw  = eval_raw.filter(lambda ex: str(ex.get(lk, "")).lower().startswith(LANGUAGE))

# Keep rows that have some text
def has_text(ex):
    key = choose_text_key(ex)
    return key is not None and isinstance(ex.get(key), str) and len(ex.get(key).strip()) > 0

train_raw = train_raw.filter(has_text)
eval_raw  = eval_raw.filter(has_text)

# Subset for smoke
train_raw = train_raw.shuffle(seed=42).select(range(min(LIMIT_TRAIN, len(train_raw))))
eval_raw  = eval_raw.shuffle(seed=42).select(range(min(LIMIT_EVAL, len(eval_raw))))

print(f"Train size: {len(train_raw)}  |  Eval size: {len(eval_raw)}")

train_ds = PiiRegexDataset(train_raw, tok, MAX_LENGTH)
eval_ds  = PiiRegexDataset(eval_raw, tok, MAX_LENGTH)
train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)
eval_loader  = DataLoader(eval_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)

# ---- Optim & sched ----
trainable = [p_ for p_ in model.parameters() if p_.requires_grad]
optimizer = torch.optim.AdamW(trainable, lr=LR)
steps_per_epoch = math.ceil(len(train_loader))
total_steps = EPOCHS * steps_per_epoch
warmup_steps = int(WARMUP_RATIO * total_steps)
scheduler = get_scheduler(name="cosine", optimizer=optimizer,
                          num_warmup_steps=warmup_steps, num_training_steps=total_steps)

model, optimizer, train_loader, eval_loader, scheduler = accelerator.prepare(
    model, optimizer, train_loader, eval_loader, scheduler
)

# ---- Train ----
for ep in range(EPOCHS):
    model.train(); tot = 0.0
    for batch in tqdm(train_loader, disable=not accelerator.is_main_process, desc=f"Epoch {ep+1}/{EPOCHS} [train]"):
        with accelerator.accumulate(model):
            out = model(input_ids=batch["input_ids"], attention_mask=batch["attention_mask"], labels=batch["labels"])
            loss = out.loss; tot += loss.detach().float()
            accelerator.backward(loss)
            if accelerator.sync_gradients:
                optimizer.step(); scheduler.step(); optimizer.zero_grad()
    if accelerator.is_main_process:
        print(f"Epoch {ep+1} train loss: {(tot/len(train_loader)).item():.4f}")

    model.eval(); totv = 0.0
    with torch.no_grad():
        for batch in tqdm(eval_loader, disable=not accelerator.is_main_process, desc=f"Epoch {ep+1}/{EPOCHS} [eval]"):
            out = model(input_ids=batch["input_ids"], attention_mask=batch["attention_mask"], labels=batch["labels"])
            totv += out.loss.detach().float()
    if accelerator.is_main_process:
        print(f"Epoch {ep+1} eval  loss: {(totv/len(eval_loader)).item():.4f}")

accelerator.wait_for_everyone()

# ---- Save merged model ----
if accelerator.is_main_process:
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    unwrapped = accelerator.unwrap_model(model)
    if isinstance(unwrapped, p.PeftModel):
        unwrapped.save_pretrained(OUTPUT_DIR)
        merged = unwrapped.merge_and_unload()
        merged.save_pretrained(OUTPUT_DIR)
    else:
        unwrapped.save_pretrained(OUTPUT_DIR)
    tok.save_pretrained(OUTPUT_DIR)
    print(f"Saved model to: {OUTPUT_DIR}")

# ================= 2) Real-time PIE Chat (Gradio, fixed API) =================
import gradio as gr
from transformers import AutoTokenizer, AutoModelForCausalLM

tok_inf = AutoTokenizer.from_pretrained(OUTPUT_DIR, trust_remote_code=True, use_fast=True)
if tok_inf.pad_token is None: tok_inf.pad_token = tok_inf.eos_token
model_inf = AutoModelForCausalLM.from_pretrained(OUTPUT_DIR, dtype=dtype, trust_remote_code=True).eval().to("cuda" if torch.cuda.is_available() else "cpu")

TAG_REGEX = re.compile(r"<([A-Z][A-Z0-9_]+)>")

def normalize_tags(text: str):
    mapping = {
        "PERSON":"NAME","NAME":"NAME",
        "EMAIL":"EMAIL","EMAIL_ADDRESS":"EMAIL",
        "PHONE":"PHONE","PHONE_NUMBER":"PHONE","MOBILE":"PHONE",
        "ADDRESS":"ADDRESS","LOCATION":"ADDRESS","STREET":"ADDRESS",
        "ZIP":"ZIP","ZIPCODE":"ZIP","POSTAL":"ZIP","POSTCODE":"ZIP",
        "SSN":"SSN","ID":"ID","IDENTIFIER":"ID","DRIVER_LICENSE":"ID",
        "DOB":"DATE","DATE":"DATE"
    }
    def repl(m):
        raw = m.group(1).upper()
        return f"<{mapping.get(raw, raw)}>"
    return re.sub(r"<([A-Z][A-Z0-9_]+)>", repl, text or "")

def mask_text(user_text: str, temperature=0.0, top_p=0.9, max_new_tokens=256):
    prompt = INSTR + "\nText: " + user_text + "\nAnswer: "
    inputs = tok_inf(prompt, return_tensors="pt").to(model_inf.device)
    with torch.no_grad():
        out = model_inf.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=(temperature>0),
            temperature=temperature,
            top_p=top_p,
            pad_token_id=tok_inf.eos_token_id
        )
    decoded = tok_inf.decode(out[0], skip_special_tokens=True)
    gen = decoded.split("Answer:",1)[-1].strip() if "Answer:" in decoded else decoded
    return normalize_tags(gen)

def count_tags(text: str):
    counts = {}
    for t in TAG_REGEX.findall(text or ""):
        counts[t] = counts.get(t, 0) + 1
    return counts

def render_pie_chat(counts: dict):
    total = sum(counts.values())
    s = ["**PII MASKING â€” PIE CHAT (session)**", f"Total <TAG> hits: **{total}**"]
    if total>0:
        s.append("\n**Category breakdown:**")
        for k,v in sorted(counts.items(), key=lambda x: x[1], reverse=True):
            s.append(f"- **{k}**: {v}  â€¢  {100.0*v/total:.1f}%")
    else:
        s.append("\n(No <TAG> placeholders yet.)")
    return "\n".join(s)

with gr.Blocks(theme=gr.themes.Soft()) as demo:
    gr.Markdown("## ðŸ”’ PII Masking â€” PIE Chat (real-time, regex-trained)")
    with gr.Row():
        with gr.Column(scale=3):
            chat = gr.Chatbot(label="Chat", height=450, type="messages")
            txt  = gr.Textbox(placeholder="Paste text with PIIâ€¦", lines=6, label="Your message")
            with gr.Row():
                send = gr.Button("Mask & Send", variant="primary")
                clear= gr.Button("Clear chat")
        with gr.Column(scale=2):
            pie_md = gr.Markdown(value="**PII MASKING â€” PIE CHAT (session)**\n(No <TAG> placeholders yet.)")
            reset = gr.Button("Reset PIE counts")

    state_counts = gr.State({})

    def on_send(messages, text, counts):
        if not text or not text.strip():
            return messages, "", counts, render_pie_chat(counts)
        masked = mask_text(text, temperature=0.0)
        new_messages = (messages or []) + [
            {"role":"user","content":text},
            {"role":"assistant","content":masked},
        ]
        cc = counts.copy()
        for k,v in count_tags(masked).items():
            cc[k] = cc.get(k,0) + v
        return new_messages, "", cc, render_pie_chat(cc)

    def on_reset(counts):
        return {}, render_pie_chat({})

    def on_clear():
        return []

    send.click(on_send, [chat, txt, state_counts], [chat, txt, state_counts, pie_md])
    reset.click(on_reset, [state_counts], [state_counts, pie_md])
    clear.click(on_clear, [], [chat])

demo.queue().launch(share=True)
