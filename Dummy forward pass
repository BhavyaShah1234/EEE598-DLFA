import torch
import torch.nn as nn
from transformers import LlavaForConditionalGeneration, SamModel, BitsAndBytesConfig

# --- Configuration ---
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f" Running on: {device}")

class LISA_HybridDummy(nn.Module):
    def __init__(self):
        super().__init__()
        
        # 1. Initialize Pretrained VLM (LLaVA-1.5 7B) in 4-bit
        print("1. Initializing VLM (Pretrained, 4-bit)...")
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16,
        )
        
        self.vlm = LlavaForConditionalGeneration.from_pretrained(
            "llava-hf/llava-1.5-7b-hf",
            quantization_config=bnb_config,
            device_map="auto",
        )
        
        # Freeze VLM backbone
        for param in self.vlm.parameters():
            param.requires_grad = False
        
        # 2. Initialize Pretrained SAM (ViT-Base)
        print("2. Initializing SAM (Pretrained)...")
        self.sam = SamModel.from_pretrained("facebook/sam-vit-base").to(device)
        
        # Freeze SAM vision encoder + prompt encoder, leave mask decoder trainable
        for name, param in self.sam.named_parameters():
            if "vision_encoder" in name or "prompt_encoder" in name:
                param.requires_grad = False
            else:
                param.requires_grad = True
        
        # 3. Bridge: LLaVA hidden_dim -> SAM prompt_dim
        print("3. Building Bridge...")
        llava_hidden_dim = self.vlm.config.text_config.hidden_size
        sam_prompt_dim = self.sam.config.mask_decoder_config.hidden_size
        
        # In real training, this comes from tokenizer.convert_tokens_to_ids("<SEG>")
        self.seg_token_id = 12345
        
        self.bridge = nn.Sequential(
            nn.Linear(llava_hidden_dim, llava_hidden_dim),
            nn.ReLU(),
            nn.Linear(llava_hidden_dim, sam_prompt_dim),
        ).to(device)

    def forward(self, input_ids, attention_mask, pixel_values, sam_pixel_values):
        print("\n--- Forward Pass Start ---")
        print(f"   [Step A] VLM Input IDs shape: {input_ids.shape}")
        
        # A. VLM forward (frozen backbone)
        with torch.no_grad():
            vlm_outputs = self.vlm(
                input_ids=input_ids,
                attention_mask=attention_mask,
                pixel_values=pixel_values,
                output_hidden_states=True,
            )
        
        # B. Extract <SEG> token embedding from last hidden state
        last_hidden_state = vlm_outputs.hidden_states[-1]  # (B, Seq, Dim)
        
        # Find index of seg_token_id in each sequence
        seg_token_indices = (input_ids == self.seg_token_id).int().argmax(dim=-1)
        print(f"   [Step B] <SEG> (ID {self.seg_token_id}) at indices: {seg_token_indices.tolist()}")
        
        batch_indices = torch.arange(last_hidden_state.shape[0], device=last_hidden_state.device)
        seg_token_embedding = last_hidden_state[batch_indices, seg_token_indices]  # (B, Dim)
        
        # C. Bridge projection to SAM sparse prompt
        sparse_embeddings = self.bridge(seg_token_embedding.float())  # (B, sam_prompt_dim)
        sparse_embeddings = sparse_embeddings.unsqueeze(1).unsqueeze(1)  # (B, 1, 1, sam_prompt_dim)
        print(f"   [Step C] Sparse prompt shape: {sparse_embeddings.shape}")
        
        # D. SAM vision encoder (frozen)
        print(f"   [Step D] Running SAM vision encoder...")
        with torch.no_grad():
            sam_features = self.sam.vision_encoder(sam_pixel_values).last_hidden_state  # (B, H', W', C)
        
        # Dense prompt embeddings = zeros (no extra hints, but correct shape)
        dense_prompt_embeddings = torch.zeros_like(sam_features)
        
        # E. SAM mask decoder: fuse image + prompt and predict mask
        print(f"   [Step E] Running SAM mask decoder...")
        sam_output = self.sam.mask_decoder(
            image_embeddings=sam_features,
            image_positional_embeddings=self.sam.get_image_wide_positional_embeddings(),
            sparse_prompt_embeddings=sparse_embeddings,
            dense_prompt_embeddings=dense_prompt_embeddings,
            multimask_output=False,
        )
        
        # Handle possible output formats
        if isinstance(sam_output, tuple):
            pred_masks = sam_output[0]
        elif hasattr(sam_output, "pred_masks"):
            pred_masks = sam_output.pred_masks
        else:
            pred_masks = sam_output
        
        return pred_masks  # (B, 1, H_mask, W_mask) or similar


# --- Execution: Dummy forward pass only ---

# 1. Instantiate model
model = LISA_HybridDummy().to(device)

# 2. Build dummy text + image inputs that respect LLaVA's image token layout
batch_size = 2
num_patches = 576  # 24x24 patches for CLIP-336
seq_len = 600      # must be >= num_patches + some text
vocab_size = model.vlm.config.vocab_size
SEG_ID = model.seg_token_id

# Random token ids
dummy_ids = torch.randint(0, vocab_size, (batch_size, seq_len), device=device)
dummy_mask = torch.ones((batch_size, seq_len), device=device)

# Inject image tokens at the start
image_token_id = model.vlm.config.image_token_index
print(f"\n Using image token ID: {image_token_id}")
dummy_ids[:, :num_patches] = image_token_id

# Inject <SEG> tokens after image tokens
dummy_ids[0, num_patches + 4] = SEG_ID   # sample 0
dummy_ids[1, num_patches + 10] = SEG_ID  # sample 1
print(f" Injected <SEG> at positions: {num_patches + 4} and {num_patches + 10}")

# Dummy images
dummy_vlm_image = torch.randn(batch_size, 3, 336, 336, dtype=torch.float16, device=device)
dummy_sam_image = torch.randn(batch_size, 3, 1024, 1024, device=device)

# 3. Forward pass
output_masks = model(dummy_ids, dummy_mask, dummy_vlm_image, dummy_sam_image)

print("\n Forward Pass Complete!")
print(f" Final Output Mask Shape: {output_masks.shape}")
